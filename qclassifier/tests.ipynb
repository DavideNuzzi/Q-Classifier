{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# from qclassifier import models, visualization, noise\n",
    "from qclassifier.models import ClassifierSingleQubitChunked, ClassifierSingleQubit, ClassifierMultiQubit\n",
    "from qclassifier.noise import DepolarizingChannel, ThermalRelaxationChannel\n",
    "from qclassifier.datasets import get_sample_data\n",
    "from qclassifier.visualization import plot_prediction\n",
    "from qclassifier.training_tools import best_initial_conditions, train, test, classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 45.35it/s, Lowest loss=0.411]\n",
      "  4%|▍         | 21/500 [00:06<02:25,  3.30it/s, Train loss=0.357, Train accuracy=0.725, Test loss=0.357, Test accuracy=0.74] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ISTC-CNR\\Desktop\\Davide\\QuantumML\\Q-Classifier\\qclassifier\\tests.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ISTC-CNR/Desktop/Davide/QuantumML/Q-Classifier/qclassifier/tests.ipynb#W1sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m model \u001b[39m=\u001b[39m best_initial_conditions(model, data_train, samples\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m, batch_size \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m, epochs \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ISTC-CNR/Desktop/Davide/QuantumML/Q-Classifier/qclassifier/tests.ipynb#W1sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m \u001b[39m1e-2\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ISTC-CNR/Desktop/Davide/QuantumML/Q-Classifier/qclassifier/tests.ipynb#W1sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m stats \u001b[39m=\u001b[39m train(model, optimizer, data_train, data_test, epochs \u001b[39m=\u001b[39;49m \u001b[39m500\u001b[39;49m, batch_size \u001b[39m=\u001b[39;49m \u001b[39m50\u001b[39;49m, patience \u001b[39m=\u001b[39;49m \u001b[39m25\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ISTC-CNR/Desktop/Davide/QuantumML/Q-Classifier/qclassifier/tests.ipynb#W1sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m accuracy \u001b[39m=\u001b[39m test(model, data_test, batch_size \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ISTC-CNR/Desktop/Davide/QuantumML/Q-Classifier/qclassifier/tests.ipynb#W1sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m Y_pred \u001b[39m=\u001b[39m classify(model, data_test[\u001b[39m0\u001b[39m], batch_size \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\Davide\\QuantumML\\Q-Classifier\\qclassifier\\training_tools.py:142\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, data_train, data_test, epochs, batch_size, patience, show_progress)\u001b[0m\n\u001b[0;32m    139\u001b[0m y \u001b[39m=\u001b[39m Y_test[(n \u001b[39m*\u001b[39m batch_size):((n\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39mbatch_size)]\n\u001b[0;32m    141\u001b[0m \u001b[39m# Prevedo le label\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m Y_pred[(n \u001b[39m*\u001b[39m batch_size):((n\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39mbatch_size)] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(x)\n\u001b[0;32m    144\u001b[0m \u001b[39m# Calcolo la loss\u001b[39;00m\n\u001b[0;32m    145\u001b[0m rho \u001b[39m=\u001b[39m model(x)\n",
      "File \u001b[1;32m~\\Desktop\\Davide\\QuantumML\\Q-Classifier\\qclassifier\\models_base.py:161\u001b[0m, in \u001b[0;36mSingeQubitClassifierBase.predict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    159\u001b[0m \n\u001b[0;32m    160\u001b[0m     \u001b[39m# Do a forward pass\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m     rho \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(x)\n\u001b[0;32m    163\u001b[0m     \u001b[39m# Calculate the probability of each class\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     p \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_fidelity(rho)\n",
      "File \u001b[1;32m~\\Desktop\\Davide\\QuantumML\\Q-Classifier\\qclassifier\\models_base.py:143\u001b[0m, in \u001b[0;36mSingeQubitClassifierBase.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mL):\n\u001b[0;32m    142\u001b[0m     layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[i]\n\u001b[1;32m--> 143\u001b[0m     rho \u001b[39m=\u001b[39m layer(rho, x)\n\u001b[0;32m    145\u001b[0m \u001b[39mreturn\u001b[39;00m rho\n",
      "File \u001b[1;32mc:\\Users\\ISTC-CNR\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Desktop\\Davide\\QuantumML\\Q-Classifier\\qclassifier\\layers.py:136\u001b[0m, in \u001b[0;36mQLayerChunked.forward\u001b[1;34m(self, rho, x)\u001b[0m\n\u001b[0;32m    131\u001b[0m U[:, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcos(phi[:, i\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m*\u001b[39m \\\n\u001b[0;32m    132\u001b[0m     torch\u001b[39m.\u001b[39mexp(\u001b[39m1\u001b[39mj \u001b[39m*\u001b[39m (phi[:, i\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m phi[:, i\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m\u001b[39m+\u001b[39m\u001b[39m2\u001b[39m])\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m    133\u001b[0m U[:, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39msin(phi[:, i\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m*\u001b[39m \\\n\u001b[0;32m    134\u001b[0m     torch\u001b[39m.\u001b[39mexp(\u001b[39m1\u001b[39mj \u001b[39m*\u001b[39m (phi[:, i\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m phi[:, i\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m\u001b[39m+\u001b[39m\u001b[39m2\u001b[39m])\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m    135\u001b[0m U[:, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msin(phi[:, i\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m*\u001b[39m \\\n\u001b[1;32m--> 136\u001b[0m     torch\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39mj \u001b[39m*\u001b[39m (phi[:, i\u001b[39m*\u001b[39;49m\u001b[39m3\u001b[39;49m\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39m-\u001b[39;49m phi[:, i\u001b[39m*\u001b[39;49m\u001b[39m3\u001b[39;49m\u001b[39m+\u001b[39;49m\u001b[39m2\u001b[39;49m])\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m    137\u001b[0m U[:, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcos(phi[:, i\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m*\u001b[39m \\\n\u001b[0;32m    138\u001b[0m     torch\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39mj \u001b[39m*\u001b[39m (phi[:, i\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m phi[:, i\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m\u001b[39m+\u001b[39m\u001b[39m2\u001b[39m])\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m    140\u001b[0m rho \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m'\u001b[39m\u001b[39mbim, bmn, bjn -> bij\u001b[39m\u001b[39m'\u001b[39m, U, rho, torch\u001b[39m.\u001b[39mconj(U))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Carico ogni dataset, creo ogni tipo di modello e uso tutte le funzioni\n",
    "datasets = ['1 circle','3 circles','one piece','mnist']\n",
    "\n",
    "for dataset in datasets:\n",
    "\n",
    "    # Carico il dataset\n",
    "    data_train, data_test = get_sample_data(dataset, N_train = 200, N_test = 2000)\n",
    "    D = data_train[0].shape[1]\n",
    "    C = len(torch.unique(data_train[1]))\n",
    "\n",
    "    # plot_prediction(data_train, data_train[1], dataset)\n",
    "    # plt.show()\n",
    "\n",
    "    # Train modello original\n",
    "    L = 2\n",
    "    depolarizing_channel = DepolarizingChannel()\n",
    "    thermal_channel = ThermalRelaxationChannel()\n",
    "\n",
    "    model = ClassifierSingleQubitChunked(D = D, L = L, C = C, noise_channels = [])        \n",
    "    model = best_initial_conditions(model, data_train, samples=50, batch_size = 100, epochs = 1)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)\n",
    "    stats = train(model, optimizer, data_train, data_test, epochs = 500, batch_size = 50, patience = 25)\n",
    "    accuracy = test(model, data_test, batch_size = 100)\n",
    "    Y_pred = classify(model, data_test[0], batch_size = 100)\n",
    "    plot_prediction(data_test, Y_pred, dataset)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize = (8,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(stats['Train loss'])\n",
    "    plt.plot(stats['Test loss'])\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(stats['Train accuracy'])\n",
    "    plt.plot(stats['Test accuracy'])\n",
    "    plt.suptitle('Original')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    model = ClassifierSingleQubit(D = D, L = L, C = C, noise_channels = [depolarizing_channel, thermal_channel])        \n",
    "    model = best_initial_conditions(model, data_train, batch_size = 100, epochs = 5)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)\n",
    "    stats = train(model, optimizer, data_train, data_test, epochs = 500, batch_size = 100, patience = 25)\n",
    "    accuracy = test(model, data_test, batch_size = 100)\n",
    "    Y_pred = classify(model, data_test[0], batch_size = 100)\n",
    "    plot_prediction(data_test, Y_pred, dataset)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize = (8,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(stats['Train loss'])\n",
    "    plt.plot(stats['Test loss'])\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(stats['Train accuracy'])\n",
    "    plt.plot(stats['Test accuracy'])\n",
    "    plt.suptitle('Improved')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    model = ClassifierMultiQubit(D = D, L = L, C = C, noise_channels = [depolarizing_channel, thermal_channel])        \n",
    "    model = best_initial_conditions(model, data_train, batch_size = 100, epochs = 5)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)\n",
    "    stats = train(model, optimizer, data_train, data_test, epochs = 500, batch_size = 100, patience = 25)\n",
    "    accuracy = test(model, data_test, batch_size = 100)\n",
    "    Y_pred = classify(model, data_test[0], batch_size = 100)\n",
    "    plot_prediction(data_test, Y_pred, dataset)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize = (8,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(stats['Train loss'])\n",
    "    plt.plot(stats['Test loss'])\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(stats['Train accuracy'])\n",
    "    plt.plot(stats['Test accuracy'])\n",
    "    plt.suptitle('Multiqubit')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
